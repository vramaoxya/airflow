ğŸš€ Overview

This project provides a modern containerized Analytics Engineering (AE) environment using:

Apache Airflow 3.1 (Scheduler, Webserver, DAG Processor)

dbt Core (CLI inside a dedicated container)

dbt Docs Server (served by NGINX)

PostgreSQL 15 (Airflow Metadata DB)

Docker Compose for orchestration

Airbyte Cloud for EL ingestion (triggered from Airflow)

This setup is designed for Data Engineering / Analytics Engineering teams who want a fully local, reproducible, cloud-ready orchestration platform.

ğŸ§± Architecture

Below is the high-level architecture of the system.

                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚      Airbyte Cloud       â”‚
                      â”‚  (Sources â†’ Destinations)â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚ API v2
                                     â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚   Airflow DAGs    â”‚
                           â”‚ trigger + monitor â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                    Docker Compose Stack                   â”‚
         â”‚                                                          â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
         â”‚  â”‚ Airflow Scheduler            â”‚    â”‚ Airflow Webserverâ”‚ â”‚
         â”‚  â”‚ - Runs tasks                 â”‚    â”‚ - UI on :8080    â”‚ â”‚
         â”‚  â”‚ - Reads DAGs                 â”‚    â”‚ - Serves logs    â”‚ â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
         â”‚                  â”‚                                          â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
         â”‚  â”‚ Airflow DAG Processor       â”‚                           â”‚
         â”‚  â”‚ - Parses DAGs               â”‚                           â”‚
         â”‚  â”‚ - Validates Python code     â”‚                           â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
         â”‚                                                              â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
         â”‚  â”‚ dbt-core                     â”‚    â”‚ dbt-docs-server  â”‚   â”‚
         â”‚  â”‚ - dbt deps/run/build/tests   â”‚    â”‚ - Serves dbt docsâ”‚   â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
         â”‚                                                              â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
         â”‚  â”‚ PostgreSQL                   â”‚                           â”‚
         â”‚  â”‚ - Airflow metadata database  â”‚                           â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


ğŸŒ URLs (Local Environment)
Component	URL
Airflow Web UI	http://localhost:8080

dbt Docs Server	http://localhost:8081
ğŸ§© Containers Overview
ğŸ”µ Airflow Scheduler

Executes DAG tasks

Triggers dbt jobs, Airbyte syncs, Python tasks

Communicates with Airflow Metadata DB (Postgres)

Talks to Docker via the docker.sock mount

ğŸ”µ Airflow Webserver

Hosts Airflow UI (http://localhost:8080)

Displays DAGs, logs, task history

Reads logs via the scheduler/log processor

ğŸ”µ Airflow DAG Processor

Parses all DAGs in /opt/airflow/dags

Ensures DAGs are valid Python code

Detects import errors

ğŸŸ£ dbt-core

CLI for dbt commands:

dbt deps

dbt debug

dbt build

dbt test

The project is mounted inside the container

ğŸŸ£ dbt-docs-server

Serves the dbt docs website

Static hosting via NGINX on port 8081

ğŸŸ¢ PostgreSQL

Stores Airflow metadata:

DAG Runs

Task Instances

Variables / Connections

Logs metadata

ğŸ“‚ Airflow DAGs Included
DAG Name	File	Description
simple_python_dag	mon_dag1.py	Basic PythonOperator example printing logs
etl_pipeline	test1_dag.py	Example ETL flow (customize for your needs)
dbt_pipeline	dbt_steps.py	Runs dbt deps + dbt build + dbt test
airbyte_ad_clicks_sync	airbyte_test.py	Triggers Airbyte Cloud sync via API v2

Each DAG is placed in:

/mnt/data/projet1/dags


And mounted in the containers at:

/opt/airflow/dags

âš™ï¸ Key Steps to Start the Project
1. Clone the repository
git clone https://github.com/vramaoxya/airflow.git
cd airflow

2. Build containers (fresh)
docker compose build --no-cache

3. Start the entire stack
docker compose up -d

4. Verify everything is running
docker compose ps

5. Open Airflow UI
http://localhost:8080

ğŸ› ï¸ Essential Docker Commands Cheat Sheet
ğŸ’  Configuration & Inspect
docker compose config

ğŸ’  Stop all containers
docker compose down

ğŸ’  Build containers without cache
docker compose build --no-cache

ğŸ’  Start containers detached
docker compose up -d

ğŸ’  Show all logs (follow mode)
docker compose logs -f

ğŸ’  Show logs per service
docker compose logs -f airflow-scheduler
docker compose logs -f airflow-dag-processor
docker compose logs -f airflow-webserver


Tail mode:

docker compose logs airflow-webserver --tail 50
docker compose logs airflow-scheduler --tail 50
docker compose logs airflow-dag-processor --tail 50

ğŸ’  Airflow DB migrations
docker compose run airflow-webserver airflow db migrate

ğŸ’  Enter containers (shell)
docker compose exec airflow-webserver bash
docker compose exec airflow-scheduler bash

ğŸ’  dbt Core commands
docker compose exec dbt-core dbt deps
docker compose exec dbt-core dbt debug
docker exec -it dbt-core dbt debug

ğŸ’  Test DAG execution manually
docker compose exec airflow-scheduler airflow dags test simple_python_dag 2025-11-24
docker compose exec airflow-scheduler airflow dags test trigger_airbyte_sync

ğŸ’  DAG diagnostics
docker compose exec airflow-scheduler airflow dags list
docker compose exec airflow-scheduler airflow dags list-import-errors
docker compose exec airflow-scheduler airflow dags delete dbt_pipeline

ğŸ”„ Data Sources & Targets (High-Level)
Sources (via Airbyte Cloud)

Typical ingestion sources:

Google Ads

Facebook Ads

HubSpot

Google Analytics

MySQL, PostgreSQL, SQL Server

File sources (S3, GCS, Azure Blob)

Your example:

ad_clicks â†’ Data Warehouse

Targets (via dbt)

Data Warehouse (BigQuery, Snowflake, Redshift, Postgres)

Transformation models: staging, intermediate, marts

Documentation & lineage (via dbt Docs)

ğŸ¯ What This Stack Enables

âœ” Modern ELT pipelines
âœ” Fully orchestrated via Airflow
âœ” dbt transformations automated
âœ” Airbyte Cloud ingestion triggered from Airflow
âœ” Automatic generation & hosting of dbt documentation
âœ” Development â†’ production reproducibility thanks to Docker

ğŸ Conclusion

This environment gives your Data / Analytics Engineering team a modern, modular, cloud-ready orchestration stack using industry standards:

Airflow for orchestration

dbt for transformation

Airbyte Cloud for ingestion

Docker for reproducibility

If you want, I can also add:

Automatic tests

CI/CD GitHub Actions

A production-ready architecture

A Makefile (build/start/test)
